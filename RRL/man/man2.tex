\chapter{Theorem Proving Methods}

\RRL supports different approaches
to theorem proving in first-order predicate calculus as well as
for proving formulas by induction. All reasoning methods in \RRL
use rewrite rules and are based on completion. In this chapter,
the key primitive reasoning operations supported in \RRL are
explained and their use in theorem proving is discussed.  Then,
an overview of various theorem proving methods supported by \RRL
is given.

Completion based approach for theorem proving in \RRL is
different from the approaches of theorem provers for first-order
predicate calculus based on the resolution method or the natural
deduction approach. \RRL is not geared towards proving one
theorem at a time. Instead, \RRL expects a first-order theory
expressed by a finite set of axioms as its input, and attempts
to generate a complete rewriting system which can
serve as a decision procedure of the theory.  
A complete rewriting system
computes a canonical form (a unique normal form) for every
congruence class induced by the theory.  It can thus be decided
whether an equation is a theorem or not by computing the
canonical forms of the two sides in the equation and checking
whether they are the same or not.

If the user however wishes to prove a single first-order formula
as a theorem, \RRL supports two kinds of methods: forward
reasoning and proof by refutation (or backward reasoning).  In
forward reasoning, besides the axiom set, the user inputs also
the theorem to be proved, \RRL will stop if that theorem is
generated solely from the input axioms.  In proof by refutation,
besides the axiom set, the user inputs the negation of the
theorem.  \RRL then attempts to generate a decision procedure for
the theory specified by the axioms and the negation of the
conclusion. During the process of generating a decision
procedure, the inconsistency of a theory is detected by examining
whether certain kinds of rules, such as {\em true $\rightarrow$
false}, are generated.  The inconsistency of this theory is
equivalent to the validity of the original formula.

\RRL currently supports two refutational methods
for first-order predicate calculus with equality.  One method,
called {\it \Groebner basis method}, is based on polynomial
representation of formulas using the exclusive-or connective as
`$+$' and the and connective as `$*'$ and using \Groebner basis
like completion procedure \cite{KN85}.  The other, called {\em
clausal superposition method}, is based on conditional rewrite
rules, which are obtained from clauses and can be used to
simplify clauses using conditional rewriting.

\RRL also supports both the {\it proof by consistency} approach
and the explicit induction approach for proving formulas by
induction.  The proof by consistency approach works only if an
equational axiomatization is sufficiently complete and a complete
rewriting system for it can be generated by the completion
procedure. The explicit induction approach does not need to have
an equivalent complete rewriting system for an equational
axiomatization.  Conditional rewrite rules are allowed in the
explicit induction approach.

\RRL has an implementation of a method using narrowing to 
solve an equation in a
given equational system. In this method, the linear completion
procedure proposed by Dershowitz is used \cite{Dershowitz85}.

In the next section, we overview the key primitive reasoning
operations in \RRL.  Then, we discuss the use of completion
procedure for forward reasoning as well as for refutational
proofs. Finally we discuss methods for proving formulas by
induction.

\section{Primitive Reasoning Operations}

There are three key operations in completion and rewrite rules based approach towards theorem proving:
\begin{itemize}
\item
making a rule from a formula or an equation,
\item
rewriting (or simplification) and 
\item
completion using critical pairs
to compensate for the loss of the bidirectionality property when
equations are transformed to unidirectional rules. 
\end{itemize}

\subsection{Making Rules from Formulas}

Any input to \RRL after simplification is converted first into
universally quantified equations and then these equations are
subsequently transformed into unidirectional terminating rules.
Properties of the interpreted functions in the formula are extensively
used for making rules. For instance, an arbitrary first-order formula
given as input is split into smaller formulas using properties of the
boolean connectives such as {\em and}, {\em or}, and {\em implies}, and then
{\em skolemized}
to eliminate the existential quantifier (see the second 
example in Appendix G).  
These skolemized formulas are converted into equations
using {\em exclusive-or} and {\em and} connectives. Rules
ensuring termination of rewriting are then made from these
equations.
  
\RRL provides algorithms based on {\em lexicographic recursive path
orderings} for comparing uninterpreted terms which are used to orient
equations into rules \cite{Dershowitz87}.  Using precedence relation on function symbols
in terms and the order in which arguments of function symbols (called
{\em status} of a function symbol) must be compared, two sides of an
equation are compared and the bigger side of the equation is made the
left side of the rule corresponding to the equation and the
smaller side is made its right side.  These orderings ensure that
the rewriting defined by rules always terminates.

The precedence and status of function symbols can be specified
before the completion procedure is invoked or this information
can be specified incrementally during the execution of the
completion procedure as the need to orient equations arises. In
an automatic mode, \RRL attempts to guess in a brute force and
exhaustive manner, a precedence relation using which all
equations during the generation of a complete rewriting system
can be properly oriented ensuring the termination of rewriting.
  
Let us consider the example of an axiomatization of free groups
in Appendix C. In case of the axiom,
    \[ e * x == x, \]
it is recognized that the right side is properly contained in the 
left side, so the equation is oriented from left-to-right.
  
In case of the second axiom,
    \[ i(x) * x == e ,\]
the precedence relation on function symbols is needed;
the axiom can be oriented from left-to-right if $i>e$ and/or
$* > e$; in the transcript, both $*$ and $i$ are declared
to have higher precedence than $e$. 
This axiom cannot be oriented from right-to-left
because if $e$ is substituted for $x$ in the left side, then the 
right side is properly contained in the instantiated left side.
  
In case of the third axiom,
     \[(x * y) * z == x * (y * z),\]
both sides have similar subterms and the same function symbol. In such
a case, it is often wise to determine what normal forms are desired,
i.e., whether normal forms should be left-associative or right-associative.
This is specified to the termination algorithm in \RRL
by setting the status of $*$ to be right-to-left or left-to-right,
respectively. If the status of $*$ is set to be left-to-right,
as is done in the transcript,
then if the outermost function symbol of the terms on the two
sides is the same, their arguments are compared from left-to-right.
If a bigger argument on the left side is found,
which is indeed the case for the first argument of the above axiom, then
the rest of the arguments of the right side are
recursively compared with
the whole left side. For the above axiom, it needs to be checked
that $((x * y)* z) > (y * z)$, which is indeed the case.
  
As is evident from the transcript in Appendix C,
the already specified precedence relation turns out to be quite
adequate for this example for orienting
new equational consequences generated except
when the equational consequence,
  \[ i(y) * i(z) == i(z * y), \]
is generated. Since there is no precedence relation between 
$*$ and $i$, this equation cannot be oriented. The precedence
relation can however be extended by making $i>*$. Then, the right side
is bigger than the left side if it can be shown that
$i(z * y) > i(y)$ as well as $i(z * y) > i(z)$;
this is indeed true. For further details about the
lexicographic recursive path ordering algorithm used in \ERRL, the reader
may refer to \cite{Dershowitz82,KaminLevy}.

For making rules from equations expressed only in terms of the boolean
connectives {\em exclusive-or} and {\em and}, two methods are
supported similar to the commonly used admissible orderings in Gr\"{o}bner
basis computation.  The goal is to pick the largest monomials
(conjunction of atomic formulas) in an equation (polynomial) and make
them the left side and the rest of the monomials as the
right side.  The default method for picking the largest monomials
is to first pick monomials of the largest size (in the number of
atomic formulas) and compare them by lexicographic recursive path
ordering; predicate symbols are ordered lexicographically in the
precedence relation unless an explicit precedence is specified.  The
second method is to use the lexicographic ordering to pick the largest
monomial ignoring the size of monomials; 
see Section 2.3.1
for details.

In the clausal superposition method, 
every clause is transformed into a conditional rewrite
rule, which can be viewed as just another presentation of a
clause.  A unit clause makes an unconditional rule while a
non-unit clause makes a conditional rule. An ordering is imposed
on literals and terms, and the head of a rewrite rule comes from
a maximal literal in the corresponding clause; see Section 2.3.2
for details.

\subsection{Rewriting}
Rewriting or simplifying terms (expressions) by rules is the basic
reasoning operation in \ERRL. An expression under consideration is
simplified using the current set of rules to obtain a normal form.  An
expression can be reduced (rewritten) if and only if there exists a
subexpression in the expression that matches the left side of a
rule, i.e., it is possible to find substitutions for variables in the
left side of the rule such that the result
of applying the substitution on the left side is
a subexpression of the expression being reduced. The result of this
single-step reduction is the expression obtained after replacing the
subexpression by the result of substituting the same values for
variables in the right side of the rule.
  
For example consider the expression $(i(x) * i(e * y)) * (e * y)$,
denoted by $t_1$; $t_1$ can be reduced using the rule corresponding to
the associativity axiom above.  
The expression $t_1$ is an instance of the left side
of the rule when
$i(x)$, $i(e * y)$ and $e * y$
are respectively substituted for $x$, $y$, $z$.
The result of rewriting $t_1$ using the
associativity rule is $i(x) * (i(e* y) * (e * y))$, which can be
further reduced using the rule corresponding to the second axiom.
  
When some of the function symbols in a rule are interpreted, then the
operation for rewriting uses matching modulo the properties satisfied
by these interpreted function symbols.  This becomes necessary when
some axioms cannot be made into rewrite rules without causing
nontermination of rewriting, for examples, when function symbols are
commutative or associative and commutative.  In certain cases, it is
more efficient for rewriting to exploit the properties of the domain
of discourse, as in the case of the \Groebner basis approach for
first-order theorem proving as well as when the domain of discourse
under consideration has $+$ and $*$ as the associative and commutative
properties and $*$ distributes over $+$.

Rewriting in the case of first-order predicate calculus turns out to
be similar to the subsumption check and demodulation used to handle
equality in resolution based proof systems. 
Conditional rewriting is more  powerful than
subsumption and demodulation together.
For more details,
the reader may refer to Section 2.3.

The facilities of making terminating rules from equations and
rewriting enable \RRL to serve as an interpreter for functional
programs written in the form of equations. In Appendix K, a recursive
definition of Ackermann's function is given which can be checked for
termination as well as completeness; \RRL can be used to execute this
definition to get the value of the Ackermann function on specific
input values.

\subsection{Superposition and Critical Pairs}

The generation of {\em critical pairs} which lead to new rules is another
crucial operation in the rewriting approach which serves as
the basis of \ERRL. Since equations are transformed into 
rules which are applied in one direction in contrast to equations
which can be applied both ways for inference, additional
rules must be added to compensate for the loss of bidirectionality
property. These new rules are generated using the concept
of {\em superposition} of a rule with itself or with other rules
which lead to critical pairs.

We illustrate these concepts with the above example. If the rules 
discussed above,
\[\begin{array}{lrll}
  1. & (i(x) * x) & \rightarrow & e, \\
  2. & (x * y) * z & \rightarrow & x * (y * z),
\end{array}\]
are considered and the first rule is superposed on the second
rule, the superposition obtained by unifying $(x * y)$ in the
left side of the second rule with the left side of the first
rule, is: \[(i(x) * x) * z.\] This term can be rewritten in two
different ways as follows:
\begin{figure}[h]
\begin{center}
 $(i(x) * x) * z$
\end{center}
\vspace*{-.15in}
\hspace*{3.0in}$\swarrow$ \hspace{0.3in} $\searrow$ \\
\hspace*{2.7in}$(e * z)$ \hspace{.5in} $i(x) * (x * z).$
\end{figure}

The pair of terms $\langle e * z, i(x) * (x * z) \rangle$
constitutes a critical pair generated by the above two rules. As
should be obvious, this pair is in the congruence relation
generated by the above two axioms.

In the case of first-order predicate calculus, critical pair of
two (conditional) rules is similar to the resolvent of two
clauses. This is discussed in more detail in later subsections.

\subsection{Completion Procedure}

The completion procedure attempts to generate new rules by
overlapping the left sides of existing rules as illustrated
above, generating critical pairs and checking whether their
normal forms are identical.  If the normal forms of two terms in
a critical pair are not identical, then this new equation is
oriented into a rule and added to the current set of rules. This
process of generating critical pairs and adding new rules is
continued until no new rule can be generated.

The completion procedure can be viewed as an implementation of an
inference system consisting of the following inference steps as
proposed by Bachmair \cite{Bachmair}.  Each inference step
transforms a pair of a set $E$ of equations and a set $R$ of
rules at any instance. The initial state is $E_0$ and $R_0$ with
$R_0$ usually being $\Phi$. An inference step transforms $\langle
E_i, R_i \rangle$ to $\langle E_{i+1}, R_{i+1} \rangle$ as
follows:

\begin{description}
\item{\bf Process an equation}
Given $e_1 = e_2$ in $E_i$, let $n_1$ and $n_2$ be, respectively,
normal forms of $e_1$ and $e_2$.
\begin{description}
\item{\em redundant equation:}
If $n_1 = n_2$, then $E_{i+1} = E_i - \{ e_1 = e_2 \}$ and
$R_{i+1} = R_i$.
\item{\em new rule:}
\begin{enumerate}
\item If $n_1 > n_2$, then  $E_{i+1} = E_i - \{ 
e_1 = e_2 \}$ and $R_{i+1} = R_i \cup \{ n_1 \rightarrow n_2 \}$.
\item If $n_2 > n_1$, then  $E_{i+1} = E_i - \{ 
e_1 = e_2 \}$ and $R_{i+1} = R_i \cup \{ n_2 \rightarrow n_1\}$.
\end{enumerate}

\item{\em new operator:} 
Let $f$ be a new function symbol not in the system, and let $x_1,
\cdots , x_j$ be the common variables appearing in $n_1$ and
$n_2$.
\begin{enumerate}
\item If $n_1 > f(x_1, \cdots , x_j)$, then  $E_{i+1} = E_i - \{ 
e_1 = e_2 \} \cup \{n_2 = f(x_1, \cdots , x_j) \}$ and \\
$R_{i+1} = R_i
\cup \{ n_1 \rightarrow f(x_1, \cdots, x_j)\}$.
\item If $f(x_1, \cdots , x_j) > n_1$, then  $E_{i+1} = E_i - \{ 
e_1 = e_2 \} \cup \{n_2 = f(x_1, \cdots , x_j) \}$ and \\
$R_{i+1} = R_i
\cup \{ f(x_1, \cdots, x_j) \rightarrow n_1 \}$.
\end{enumerate}
\end{description}

\item{\bf Add critical pairs}
Given two (not necessarily distinct) rules $l_1 \rightarrow r_1$
and $l_2 \rightarrow r_2$ in $R_i$, $R_{i+1} = R_i$\\
\hspace*{1.0in}and $E_{i+1} = E_i \cup
\{ c_1 = c_2 \mid \langle c_1, c_2\rangle$ is a critical pair of $l_1 \rightarrow r_1$
and $l_2 \rightarrow r_2 \}$.

\item{\bf Normalize rules}
Given two distinct rules $l_1 \rightarrow r_1$ and $l_2
\rightarrow r_2$ in $R_i$,
\begin{enumerate}
\item if $l_1 \rightarrow r_1$ rewrites $l_2$, then $E_{i+1} = E_i \cup
\{ l_2 = r_2 \}$ and $R_{i+1} = R_i - \{ l_2 \rightarrow r_2 \}$.
In this case, the rule $l_2 \rightarrow r_2$ is deleted from
$R_i$ and inserted as an equation into $E_i$.
\item if $l_1 \rightarrow r_1$ rewrites $r_2$, then $E_{i+1} = E_i$
and $R_{i+1} = R_i - \{ l_2 \rightarrow r_2 \} \cup \{ l_2
\rightarrow r'_2 \} $, where $r'_2$ is a normal form of $r_2$
using $R_i$.
\end{enumerate}
\end{description}
As stated above, in the implementation of the completion
procedure of \ERRL, the ordering $>$ is incrementally extended;
thus above $n_1$ and $n_2$ may be not comparable under the
existing ordering, but could be compared by extending the
ordering. \RRL also provides other choices to the user for
processing equations; in particular, the user can choose to
postpone considering % a large equation or an equation that
cannot be oriented using an ordering in use. This option is quite
helpful as there are examples of equational theories for which it
is not known whether complete rewriting systems can be generated
without postponing intermediate equations generated during
completion.

The above steps can be combined in many different ways to
generate a complete system when all critical pairs among rules
have been considered or until an inconsistency is detected.  \RRL
supports different strategies for generating critical pairs,
normalizing terms, as well as processing critical pairs; these
strategies can be selected by setting appropriate flags and
parameters in \ERRL.

\section{Generating Decision Procedures and Forward Reasoning}

One of the main objectives of \RRL has been to experiment with
completion procedures for dynamically generate decision
procedures for equational axiomatizations. \RRL is primarily
tuned to this kind of usage and theorem proving using forward
reasoning. Given an equational axiomatization, \RRL can be used
to generate a complete rewrite system which serves as the
decision procedure for the equational axiomatization.

For example, for the classical axiomatization of free groups
using the three axioms stated above, \RRL generates the following
complete rewriting system:  
\[\begin{array}{rcl}
   (e * x) & \rightarrow & x, \\
   (i(x) * x) & \rightarrow & e, \\
   ((x * y) * z) & \rightarrow & (x * (y * z)), \\
   (i(y) * (y * z)) & \rightarrow & z, \\
   (z * e) & \rightarrow & z, \\
    i(e) & \rightarrow & e, \\
    i(i(y)) & \rightarrow & y, \\
   (y * i(y)) & \rightarrow & e, \\
   (y * (i(y) * z)) & \rightarrow & z, \\
    i((y * z)) & \rightarrow & (i(z) * i(y)).
\end{array}\]
The above ten rules serve as a decision procedure for the
equational theory of free groups; the details about how these
rules are generated from the original three axioms by the
completion procedure are given in a transcript in Appendix C. To
check whether an equation is in the equational theory or not, it
is sufficient to compute the normal forms of the two sides of the
equation and check whether the normal forms are the same or not.

The completion procedure in \RRL allows function symbols to be
commutative or associative and commutative. Appendix E includes a
transcript illustrating how \RRL can be used to generate a
complete rewriting system for an abelian group. Function symbols
which only satisfy the associative law are not handled in any
special way; the associative law is transformed into a rewrite
rule just like other equational axioms. \RRL also has a
special-purpose completion procedure for handling rings
(algebraic structure with two operators, $+$ and $*$, in which
$+$ is associative and commutative, $*$ is associative, and $*$
distributes over $+$ on the left as well as on the right).

Given that the completion procedure may not terminate in general,
conjectures can be tried using \RRL by forward reasoning by the
method of proof by completion. The conjecture being tried is
checked incrementally whenever a new rule is generated. The proof
of a conjecture thus does not require that a complete rewriting
system be generated first; see Appendix F for an example.

\section{Refutational Methods in First-order Predicate Calculus}

\RRL supports two methods for first-order deductive reasoning. Both
methods are refutational and use completion. The first method is
nonclausal and is based on converting formulas into polynomials
in which $+$ is the exclusive or connective and $*$ is the and
connective (written as $\&$). The completion procedure is an extension of
Buchberger's \Groebner basis algorithm to first-order polynomials
as discussed in \cite{KN85}.  Rewriting and superposition are
defined making use of the nilpotency property of $+$ and the
idempotency property of $*$.

The second method uses conditional rewrite rules \cite{ZK88}.
It assumes input in clausal form or in form of conditional equations. 
A maximal literal of a clause is used for rewriting with the
remaining literals serving as the context in which the rewriting
is applicable. Clausal superposition is defined on conditional
rewrite rules. Below, we provide an informal overview of the two
methods. For further details, the reader may consult the papers
in which the methods were first proposed.

Unlike resolution based theorem provers,
\RRL does not support any strategies especially geared towards refutational
theorem proving. Instead strategies used for generating complete
rewrite systems are employed even for refutational theorem
proving.  As a result, certain proofs take considerable time
while they could be easily obtained using proper strategies.

\subsection{\Groebner Basis Method}

This method takes as input the negation of a formula being
checked for validity. If a formula has the form of a set of
hypotheses implying a conjecture, then the hypotheses and the
negation of the conjecture are given as the input.

Each input formula is equated to $true$ and converted into a
polynomial equation form. This is done by exploiting the
semantics of the outermost logical connective in a formula.
Whenever possible, a formula is split into smaller formulas which
would result in smaller polynomial equations.  (\RRL can also
introduce new propositional variables to control the blow-up
which will arise in converting a disjunction of formulas into a
polynomial form.)

Skolemization is performed as needed to eliminate existential
quantifiers. Once an equation equivalent to a formula is
generated, the equation is simplified using the current set of
rules already generated. A rule corresponding to the equation is
made by making the maximal product (monomial) of atomic formulas
in the polynomial equation as the left side of the rule and
remaining polynomial as the right side of the rule.  A
well-founded partial ordering on atomic formula is extended to
products (sets) of atomic formulas and polynomials (sets of sets
of atomic formulas). Since this ordering is partial, there can be
more than one maximal products in a polynomial; in that case, all
maximal products constitute the left side of a rule.

For example, consider the following subset of formulas in
propositional calculus taken from an example discussed in
Appendix G.

\[\begin{array}{rl}
  1. & (not(e)~and~not(k))~implies~r,  \\
  2. & (not(r)~and~(not(m)~and~k))~implies~false, \\
  3. & (not(e)~and~r)~implies~not(a). 
\end{array}\]

These formulas are equated to $true$ and then the equations are
converted to polynomial equations one by one. The first gives:
  \[  e \& k \& r + e \& k + e \& r + k \& r + e + k + r == true.\]
The rule for this polynomial equation is:
  \[ 1. \; \; e \& k \& r \rightarrow e \& k + e \& r + k \& r + e + k + r + true.\]
Similarly second and third formulas above give the rules:
  \[~~~~~~~~\; \; \; \; \; \; \; 2. \; \; k \& m \& r \rightarrow k \& m + k \& r + k,\]
  \[ 3. \; \; a \& e \& r \rightarrow a \& r. \]

Rewriting by a rule is defined as follows: If a polynomial has a
monomial in which the left side of the rule appears (i.e., the
monomial is a product of the left side), then that monomial in
the polynomial is replaced by the result obtained by multiplying
the right side of the rule by quotient obtained by dividing the
monomial by the left side. After the replacement, idempotency of
$\&$ and nilpotency of $+$ are used to simplify the result.

There are two kinds of superpositions: superpositions between two
distinct rules and {\it idempotent} superpositions of a single
rule. For example, superposing rules 1 and 3 results in the
superposition 
   \[a \& e \& k \& r, \]
the {\it least common multiple} of the
left sides of rules 1 and 3, and the critical pair is 
  \[ \langle a \& e \& k + a \& e \& r + a \& k \& r + a \& e + a \& k + a \& r + a, a \& k \& r\rangle . \]
The corresponding S-polynomial is the sum of the two polynomials
in the critical pair: 
 \[ a \& e \& k + a \& e \& r + a \& e + a \& k + a \& r + a,\]
which is normalized using the above three rules.
Using rule 3, we obtain 
  \[a \& e \& k + a \& r + a \& e + a \& k + a \& r + a,\]
which simplifies to $a \& e \& k + a \& e + a \& k + a$ thus
giving a new rule: 
  \[ 4. \; \; a \& e \& k \rightarrow a \& e + a \& k + a.\] 
This is similar to resolving the clauses for formulas 1
and 3.

The idempotency superposition can be best illustrated on the
rule: 
  \[ p \& q \rightarrow p + q.\] 
The idempotency critical
pair is defined as a pair of polynomials whose first component is
the right side of the rule and the second component is the
polynomial obtained by multiplying the right side by an atomic
formula appearing in the left side. For a given rule, there is an
idempotency critical pair for every atomic formula in its left
side. For the above rule, one idempotency critical pair is: 
   \[\langle p + q, p + p \& q\rangle ; \]
the S-polynomial is $p \& q + q$ whose normal
form is $q$. This gives a new rule $q \rightarrow false$. There
is another idempotency critical pair due to $q$: $\langle p + q, p \& q
+ q\rangle $; however, this will not be considered since the new rule
will delete the above rule.

If during the completion, a rule $true \rightarrow false$ is
generated, this implies the set of input formulas is
inconsistent. If the completion procedure stops
(which is guaranteed to happen in the case of propositional calculus)
without generating an inconsistency,
the complete system thus
generated (also called a \Groebner basis) is a decision procedure
for the input theory.

As stated earlier, \RRL supports two orderings for picking
maximal products -- degree ordering and lexicographic ordering. In
the lexicographic ordering, atomic formulas in products are
compared lexicographically (the default is to use the dictionary
ordering on predicate symbols).  In the degree ordering, the
degrees of products are first compared and in case two products
have the same degree, then the atomic formulas in products are
compared lexicographically.

For first-order formulas, quantifiers are removed during the
translation to the polynomial form. Rules are made just as in the
case of propositional calculus. Substitutions are used to relate
similar atomic formulas (formulas sharing the same predicate
symbol).  Rewriting is defined using a substitution for variables
in the left side of a rule. Superpositions are also defined using
substitution by unifying similar atomic formulas and then taking
the least common multiple of the left sides.  The method is
illustrated using formulas picked from an example discussed in
Appendix G.
 \[ all~z (all~y ((isequal(y, z)~equ~all~x ((member(x, y)~equ~member(x, z)))))), \]
 \[ not(all~x (all~y ((isequal(x, y)~equ~isequal(y, x))))), \]
where $equ$ is the equivalence connective. The first formula is split
into two subformulas which result in the following two rules:
 \[ ~~ 1. \; \; (isequal(y, z) \& member(x, y) + isequal(y, z) \& member(x, z)) \rightarrow false, \]
 \[ 2. \; \; (member(s_1(y, z), y) + member(s1(y, z), z)) \rightarrow true + isequal(y, z), \]
where $s_1(y, z)$ is a skolem function for the variable $x$
in the subformula 
\[ all~x ((member(x, y)~equ~member(x, z))).\]
Notice that rule 1 above has two monomials on its left side since
there are two maximal products.
Similarly, using Skolem constants $s_2$ for $x$ and 
$s_3$ for $y$ in $not (all~x (all~y ((isequal(x, y)~equ~isequal(y, x)))))$,
the third rule is generated:
 \[3. \; \; (isequal(s_2, s_3) + isequal(s_3, s_2)) \rightarrow true.\]

Rules 1 and 3 superpose using the most general unifier for
similar atomic formulas $isequal(y, z)$ and $isequal(s_2, s_3)$ and
the superposition is: 
 \[ (isequal(s_2, s_3) \& member(x, s_2) + isequal(s_3, s_2) \& member(x, s_2) + isequal(s_2, s_3) \& member(x, s_3)). \]
The critical pair is: 
\[\langle isequal(s_3, s_2) \& member(x, s_2) + false, member(x, s_2) + isequal(s_2, s_3) \& member(x, s_3)\rangle ,\] 
which gives the rule:
 \[ 4. \; \; isequal(s_3, s_2) \& member(x, s_2) + isequal(s_2, s_3) \& member(x, s_3) \rightarrow member(x, s_2).\]
There are also idempotency critical pairs including the ones
arising because atomic formulas in a monomial on the left side of
a rule may unify.  This process of generating critical pairs and
new rules is continued until a contradiction is generated or 
the completion procedure terminates. 

Unit clauses which are equations result in rewrite rules in the
same way as while applying completion on equational theories.

There are superpositions among rules in which
the left side includes equations as atomic formulas;
these superpositions have the effect of paramodulation rule
of inference used in resolution-based systems for handling equality.
Consider the following two rules:
  \[5. \; \; P(f(x)) \& Q(g(x)) ~\rightarrow~ P(f(x)), \]
  \[~~~~~~~~~~~~~6. \; \; eq(f(a(x)), g(b(x))) \& Q(g(x)) ~\rightarrow ~ Q(g(x)).\]
The critical pair due to unifying the subterm $f(x)$ in rule 5
with the left side
of the equality, $eq(f(a(x)), g(b(x)))$, in the left side of rule 6 is:
  \[\langle Q(g(x)) \& (P(g(b(x))) \& Q(g(a(x))) ~+~P(g(b(x)))), ~false \rangle. \]
However, superposition between rule 5 and the following rule:
   \[7. \; \; f(c(y)) ~\rightarrow~ g(d(y)), \]
is defined simply using the most general unifier of $f(x)$ and $f(c(y))$,
which gives the critical pair:
    \[\langle P(f(c(y))), ~P(g(d(y))) \& Q(g(c(y))) \rangle.\]

A critical pair due to paramodulation is defined as follows:
  \[a. ~ P( \cdots, t_1, \cdots) \& h_1 ~\rightarrow~ R_1 \&  P( \cdots, t_1, \cdots) ~+~ {R'}_1, \]
  \[b. ~ eq(t, s) \& h_2 \rightarrow~ R_2 \& eq(t, s) ~+~ {R'}_2, \]
where ${R'}_1$ and ${R'}_2$ do not include, respectively, 
$P( \cdots, t_1, \cdots)$ and $eq(t, s)$, and $t$ unifies with a nonvariable
subterm ${t_1}/p$ of $t_1$ by a most general unifier $\sigma$. 
The critical pair is:
 \[ \langle \sigma({R'}_2) \& \sigma(P( \cdots, t_1 [p \leftarrow s ], \cdots) ( h_1 ~+~ R_1 ) ~+~ {R'}_1 ), ~0 \rangle . \]

The method has been successfully used on many examples including
Schubert's steamroller example, puzzles form Lewis Caroll's books
and Smullyan's books and other challenge problems appearing in
the newsletter of Association of Automated Reasoning and in
Journal of Automated Reasoning. All first-order proofs needed in
hardware verification application of \RRL were done using this
method. 

\subsection{Clausal Superposition and Conditional Rewriting}

The clausal superposition method is a restriction of resolution
and paramodulation by imposing a well-founded ordering on terms
as well as atoms. In addition, conditional rewriting is used as a
simplification rule of inference.  The method assumes the input
to be in a clause form; thus it is assumed that Skolemization has
already been done to eliminate quantifiers.

Every clause is transformed into a conditional rewrite rule (or a
finite set of rewrite rules), with a maximal literal being the head
of the rule and the negation of the remaining literals being the
condition or the premises of the rule. A well-founded ordering on
terms and atoms is used to select maximal literals in a clause.

Conditional rules are superposed to generate new rules (or
clauses).  This definition of superposition between two rules
subsumes both resolution and paramodulation on the maximal
literals of the corresponding two clauses. In this sense,
resolution and paramodulation can be treated uniformly.  The
reason for calling this method as clausal superposition is to
emphasize that the rules are made from clauses and the result of
a superposition is also a clause.

Conditional rules are also used to simplify other rules.  If a
clause gets reduced, the new clause is kept and the old is thrown
away.  This reduction is more powerful than subsumption and
demodulation together.

The input to the method could be
                   \[ p(a) ~or~ not~ q(b)~ or~ (a = b), \]
or equivalently, the conditional equation,
                   \[ p(a) == true ~{\bf if} ~ q(b) ~and ~ not (a = b). \]
\RRL will transform a clause or a conditional equation into
a conditional rewrite rule. For making a rewrite rule 
from a clause, it is better to 
think of each literal as an equation and a clause as
a disjunction of equations. For example, the above clause can be considered as
                   \[ (p(a) = true) ~or~ (q(b) = false) ~or~ (a = b). \]
\RRL chooses a maximal 
equation as the head of the rule and puts the negation 
of the remaining equations into the condition of the rule.
For instance, if an ordering on atoms
is chosen in which $q(b) > p(a) > b > a$, then 
the above clause gives the following rule
              \[ ~~~~~~~~~1.  ~~~ q(b) \rightarrow false~ {\bf if} ~\{ p(a) \Leftrightarrow false,~  (a = b) \Leftrightarrow false \}. \]

As in the case of equational theories,
\RRL uses the lexicographic recursive
path ordering to compare terms and atoms.  If more than one
literal in a clause are maximal, \RRL chooses only the first
maximal literal as the head of the rule (this makes the method
incomplete in general). Hence, it is always helpful if the user
specifies a precedence order on predicate and function symbols.

Superposition of rules is similar to resolution and
paramodulation. For example, the superposition of the rule 1
above with the following rule,
     \[  2. ~~~ q(b) \rightarrow true  ~{\bf if}  ~\{ p(b) \}, \]
is:
                   \[ p(a) ~or~ (a = b) ~or~ not~ p(b). \]

Superposition of rule 1 with the following rule,
         \[3. ~~~    b \rightarrow c ~ {\bf  if} ~ \{ a = b \}, \]
is:
          \[    not~ q(c) ~or~  p(a) ~or~ (a = b) ~or~ not (a = b), \]
which is trivial after simplification. 
This superposition is equivalent to a paramodulation.

The conditional rewriting used in \RRL is {\em contextual
rewriting} described in \cite{ZK88,Zhang88}.  This rewriting is
illustrated using a simple example.  A clause,
   \[ not~ istiger(y) ~or~ not ~isanimal(y),\]
can be simplified using a rule
   \[ isanimal(x) \rightarrow true ~{\bf if}~ istiger(x). \]
To simplify $not ~isanimal(y)$, the remaining literal of the clause,
$\{ not ~istiger(y) \}$, is first negated to give
$\{ istiger(y) = true \}$, 
which is called the {\em context} of the literal $not~isanimal(y)$.
The left side of the rule, $isanimal(x)$, is matched against
$isanimal(y)$ using the substitution $\sigma = \{ x / y \}$. The
condition of the rule under $\sigma$ is $istiger(y)$, which can
be simplified to $true$ by the context, i.e, by $\{ istiger(y) =
true \}$.  So the rule is applicable and $isanimal(y)$ is reduced
to $true$.  The clause is simplified to $not~istiger(y)$. Note
that neither subsumption nor demodulation can simplify the above
clause to $not ~istiger(y)$.

The idea of using the remaining literals in a clause as the
context to simplify one literal was first described in
\cite{BoyerMoore79}.  A formal study of this rewriting is given
in \cite{ZK88,Zhang88}, where it is shown that this rewriting can
apply not only to inductive theorem proving, but also to
deductive theorem proving when viewing each clause as a
(conditional) rewrite rule.

The clausal superposition integrated with conditional rewriting
method has been implemented in \RRL and has been tried on a
number of examples and results are extremely encouraging. Some
experimental results have been presented in \cite{ZK88} where the
method has been demonstrated on a number of examples including
Schubert's Steamroller problem, SAM's lemma, and some examples
from set theory. The clausal superposition method works
particularly well if the input involves non-horn clauses with
equality.

\section{Proof by Consistency and Induction}

Along with the forward reasoning and proof by refutation
approachs used for proving first-order formulas, \RRL also
supports methods for proving inductive properties (or inductive
theorems) from an algebraic specification of abstract data types.

An algebraic specification of abstract data types consists of a set of
functions with proper type (or sort) declaration and a set of
equations characterizing these functions.  The approach adopted in
\RRL for specifying an abstract data type using equations is to
explicitly divide all functions of an abstract data type into two
disjoint classes: {\em constructor} and {\em nonconstructors}, with
the interpretation that the constructors and equations on constructors
define the model of the data type.  An equation is said to be an {\em
inductive property} or {\em inductive theorem} of the data type
if any instance of the equation, which is obtained by substituting the
variables in the equation by a ground term containing only constructors, can
be proved to be true by first-order predicate calculus.  In other
words, an inductive property is an ``abstraction'' of many theorems
provable by forward reasoning or by refutational proof.

Proving inductive properties are very important to study, design, implement and
verify algebraic specifications. Currently, \RRL supports two approachs
for proving inductive properties: proof by consistency and 
proof by induction based on well-founded orderings.

\subsection{Proof by Consistency}

As the name implies, this method is opposite of the
the refutational (or proof by contradiction) approach;
the equation being proved is assumed
and it is checked whether the augmented theory
(the theory specified by the union of
the axioms plus the theorem to be proved) is consistent.  
The inconsistency in the proof-by-consistency approach is
detected by checking whether certain constructor values
declared to be {\em unequal} have
been made equal as a result of an equation being added.

In this approach, an equation to be proved by induction is added to 
the specification; the completion procedure attempts to generate a decision
procedure for the augmented theory.  During the process of generating
this decision procedure in the form of a complete rewriting system, if
an inconsistency is generated, then \RRL backtracks and notifies that
the equation being proved is not an inductive theorem.  Otherwise, if
no inconsistency is detected, then the equation is an inductive
theorem. 
This approach, which is popularly known as the inductionless induction
method, is radically different from the conventional induction method
in which the induction is done explicitly on a well-founded ordering
by considering the basis step and the inductive step.  

Two inductionless induction methods have been implemented in \ERRL.
One method, proposed in \cite{JK86}, is based on the concept of {\em
ground reducibility} (also called {\em quasi-reducibility} or {\em
inductive reducibility}).  A term is {\em ground-reducible} if and
only if every ground instance of this term is reducible.  The
consistency of the extended theory is equivalent to the property that
the left side of rewrite rules generated when the completion procedure
is run on the extended theory is ground-reducible.  \RRL supports a
method to decide the ground reducibility of a term in left-linear
systems (the occurrence of each variable in the left side of each rule
is unique).  When a new rule is generated in the completion procedure,
the method checks that the left side of the rule is ground reducible.
This method has an advantage over the methods proposed by Musser,
Goguen, and Huet and Hullot as it allows to specify relations between
constructors.

We also implemented 
another method, proposed in \cite{KNZ86}, based on the concept of 
{\em test set}.
A {\em test set} is a finite set of terms which describes the equivalence
classes of constructor ground terms. For an equational theory with an
associated canonical rewriting system, each equivalence class can be
represented by a canonical element which is the normal form of every
element in the equivalence class with respect to the rewriting system.
To check whether a conjecture follows by induction from an equational
theory with an associated canonical rewriting system, the test set
method involves computing a canonical rewriting system for the
equational theory augmented with the conjecture.  This is done
incrementally using the already computed canonical rewriting system of
the equational theory and the rule corresponding to the conjecture. In
this process, whenever a new rule is generated, it is checked whether
it reduces an irreducible constructor ground term by computing a new
test set and testing its equivalence with the old test set. 
This method is theoretically as powerful as the first method,
since the relations between constructors are allowed.
We illustrate the method by a simple example where 
a unary function $neg$ is defined on integers.

In the case of integers, the
constructors are $0$, ${suc}$ and ${pre}$, and the relations
among them are:
 \[\begin{array}{lcl}
  suc(pre(x)) & = & x, \\ 
  pre(suc(x)) & = & x. \\
 \end{array}\]
Integers can be represented by $0$, $suc^i(0)$, $i > 0$, for positive
integers, and $pre^j(0)$, $j > 0$, for negative integers. An
equational theory defined on integers is inconsistent if and only if
two distinct integers are made equivalent in it.
A unary function $neg$ is defined on integers, giving the negative of 
its argument.
\[\begin{array}{rlcl}
  1. & suc(pre(x)) & \rightarrow & x, \\
  2. & pre(suc(x)) & \rightarrow & {x}, \\ 
  3. & neg(0) & \rightarrow & {0}, \\
  4. & {neg}(suc(x))  & \rightarrow & pre(neg(x)).
\end{array}\]
Using rules 1 and 4, a new rule is generated: 
  \[ {5.} ~~~ pre(neg(pre(x))) ~~ \rightarrow ~~ neg(x). \]
 From rules 2 and 5, another rule is generated:
  \[ {6.}~~~ neg(pre(x)) ~~ \rightarrow ~~ suc(neg(x)), \]
which deletes rule 5. Rules $\{1, 2, 3, 4, 6 \}$ 
constitute a canonical rewriting system. Further, this also
establishes that the function symbol $neg$ is completely defined. The
set of irreducible constructor ground terms is 
$\{ 0, suc^i(0), pre^j(0) \mid i > 0, j > 0 \}$. 
The test set for this example is: 
$\{ 0, suc(0), suc(suc(x)), pre(0), pre(pre(y))
\}$.

To check whether $neg(neg(x)) = x$ can be proved by induction, 
we add the conjecture as another rule:
 \[ 7.~~ neg(neg(x)) ~~\rightarrow~~ {x.}\]
It turns out that the rule set $\{1, 2, 3, 4, 6, 7 \}$ is a canonical
rewriting system for the augmented theory. Since its test set is the
same as the original test set, the conjecture is indeed a theorem by
induction.

Now consider another conjecture which is obviously not a theorem:
  \[ neg(neg(neg(x))) = x, \]
since it is not true if $x$ is different from $0$;
we are not assuming $neg(neg(x)) = x$ as a theorem.
If we add the conjecture as a rule:
 \[ 8. ~~~ {neg}(neg(neg(x))) ~~\rightarrow ~~ x,\]
we get new rules from rules 4 and 8:
 \[ 9. ~~~ {pre}(x) ~~\rightarrow~~ suc(x). \]
Rule 9 reduces the term $pre(pre(y))$ in the test set,
which means that at least two distinct irreducible constructor ground
terms were made equivalent by the conjecture. This implies that the
conjecture is not a theorem.  By instantiating the term reduced in the
test set, we also get a counter-example, $pre(pre(0))$, to
the conjecture.  In fact, we obtain an infinite family of
counter-examples.

The main advantage of this method is that it is a semi-decision procedure
for determining whether a conjecture is {\sl not} an inductive theorem.
Further, as illustrated above, for a false conjecture, a
counter-example can also be determined using this method which
will eventually be generated while
attempting to generate a canonical rewriting system for the augmented
theory.

The method works quite well on simple examples, especially when
functions are defined using constructors in primitive recursion style.
In those cases, it is often possible to generate a canonical rewriting
system from the definitions, and from the definitions augmented
with the conjecture to be proved. The method, if it works, does not
need much user interaction.

As to be expected, the proof-by-consistency approach is not always
sound; it is valid only if the algebraic specification is {\em
sufficiently complete}, which requires that every nonconstructor
function be completely defined on every constructor terms.
\RRL provides algorithms for checking the sufficient completeness property.
If the user does not specify the set of constructors, then the
specification is assumed to be sufficiently complete as every function
is assumed to be a constructor.  We have developed a method for using
the proof-by-consistency approach for ambiguous theories also but this
method has not yet been implemented in
\ERRL; an interested reader may wish to consult \cite{KapurMusser84}.

\subsection{Cover Set Induction Method}

The proof by consistency method is not widely applicable. When
functions are not defined directly using constructors and are instead
defined using other already defined functions (see the ${gcd}$ example
below), there are two problems that may arise.  First, it may not be
possible to obtain a finite canonical rewriting system (or even a
ground confluent rewriting system) from the definitions, as one simply
may not exist.  Secondly, even if a canonical rewriting system for the
definitions exists, it may not be possible to generate a finite
canonical rewriting system (or even a ground confluent rewriting
system) when the conjecture being proved is also included.

We have implemented in \RRL a method based on the explicit
induction approach for automating proofs by induction. The method,
called the {\sl cover set} method, is reported in \cite{ZKM88,Zhang88}. 
It generalizes Burstall's structural induction method
and is closely related to Boyer and Moore's approach. Using the
cover set method and conditional
rewriting.
\RRL can be used to prove nontrivial 
theorems including the unique prime factorization theorem of numbers.
Below, we illustrate the cover set method using an example over
natural numbers, ${Nat}$, whose constructors are $0$ and ${suc}$. The
function $+$ is defined on ${Nat}$ in a primitive recursive style:
\[\begin{array}{rlcl}
 1. & x + 0 & = & x, \\
 2. & x + suc(y) & = & suc(y).
\end{array}\]
Another function $gcd$ can be defined on ${Nat}$ as follows. Notice
that this definition is not given using the constructors; instead, it
makes use of the function $+$.
\[\begin{array}{rlcl}
  3. & gcd(x, 0) & = & x, \\
  4. & gcd(0, x) & = & x, \\
  5. & gcd(x, x + y) & = & gcd(x, y), \\
  6. & gcd(x + y, y) & = & gcd(x, y).
\end{array}\]

To prove theorems about ${gcd}$ using induction, it turns out
that Peano's induction rule, or even the more general structural
induction principle due to Burstall does not help,
because the proof subgoals produced by these theorems may not be
provable by equational inference. Let us check for instance whether
  \[gcd(x, y) = gcd(y, x)\]
using structural induction. Without any loss of generality, we can
induct on $x$.  One of the subgoals is to prove that
  \[ gcd(0, y) = gcd(y, 0), \]
which follows by equational inference from the above definitions. The
second subgoal is to prove that
 \[ gcd(x, y) = gcd(y, x) \Rightarrow
    gcd(suc(x), y) = gcd(y, suc(x)), \]
which is not provable by equational reasoning from the above
definitions. In fact, the second subgoal is similar to the original
conjecture and is no easier to prove.

The test set method does not work either for the $gcd$ example,
because the above equational theory constituted from the definitions
of $+$ and $gcd$ does not have an equivalent finite canonical
(or ground confluent) rewriting system.

The cover set induction method for equational theories is an
attempt to overcome these limitations.  It is similar to Boyer and
Moore's approach in which inductive inference rules are designed from
recursive definitions of functions.  There is no fixed inductive
inference rule for a data structure; instead the cover set induction
method is used to design an inductive inference rule based on a set
of equations defining a function.  Different types of definitions of
functions can lead to different inductive inference rules for the
same data structure.  Because the structure of these generated rules
mirrors the structure of the defining axioms, they often yield
subgoals that are provable with equational inference.  

For the above example, for properties about $+$, the cover set method
proposes an inductive inference rule which is the same as the
structural induction rule using constructors $0$ and $suc$, but
for properties in which $gcd$ appears, the inductive inference
rule could be stated using the function symbol $+$.  For example, for
proving $gcd(x, y) = gcd(y, x)$, a cover set for
$gcd$ is obtained from its definition (assuming that
$gcd$ is completely defined) which is:
 \[(0, x), (x, 0), (x + y, y), (x, x + y) .\]
A cover set is in general a finite set of terms (or $n$-tuples of terms
for functions with arity $n$) with the property that every ground term
(or $n$-tuple of ground terms) representing the values (or $n$-tuples of
values) of the data type is equal to an instance of one of the
terms (or $n$-tuple of terms) in the cover set.  Instead of
having a fixed inductive inference rule or set of such rules, one
generates new inference rules based on the structure of the
equational axioms used to define the functions of the language.

 From the above cover set, we get the following inductive inference rule for
natural numbers:
 \[\begin{array}{ccc}
& P(x, 0) &\\
   & P(0, y) &\\
   P(x, y) & \Rightarrow & P(x + y, y)\\
   P(x, y) & \Rightarrow & P(x, x + y)\\
   \hline
   & P(x, y) &
 \end{array}\]
where $P$ is a formula with two free variables ranging over the
natural numbers. Using this inductive inference rule, the conjecture
${gcd}(x, y) = {gcd}(y, x)$ can be easily proved using the
definitions of $+$ and ${gcd}$ given above as Equations 1 to 6.

It is important that the cover set thus defined be complete in the
sense that for every ground term representing the values of the data
type is an instance of some term in the cover set. For the above
example, it must be shown that the function $+$ is onto. \RRL
currently does not provide any method for checking whether a cover set
is indeed complete.

Theoretically, the cover set induction method is more powerful than
the induction principle in Boyer and Moore's approach, in two important
aspects: it works even when some functions are incompletely defined,
and when there are relations between constructors; for more details,
see \cite{ZKM88,Zhang88}.

Although the cover set method is quite powerful, it is not easy to use
for obtaining proofs. It is primarily user-driven, as it requires
considerable user assistance in the form of key lemmas needed to push
a proof of a theorem through \ERRL.  In that sense, the method is not
completely automatic.  User interaction is also required in converting
the problem formulation and lemmas into conditional rewrite rules with
the termination property.
Another drawback of the cover set method is that when it fails, it is
often not clear why it failed: whether the conjecture being proved is
false or a proof of the conjecture needs additional lemmas.  The
method does not provide any help to the user in the case of failure.


